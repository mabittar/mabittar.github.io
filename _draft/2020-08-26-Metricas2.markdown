---
layout: post
title: Métricas para Avaliação de modelos de Machine Learning
date: 2020-08-26 00:00:00 +0300
description: Avaliações e Comparação de modelos de Machine Learning # Add post description (optional)
img: curve.jpg # Add image post (optional)
tags: [Machine Learning, Python, Colab, Google, Data Analysis] # add tag
---

Essa é a segunda parte do [post](_posts/2020-08-26-Metricas.markdown) referente a métricas para avaliação dos modelos de Machine Learing.
No primeiro post foi abordado alguns temas sobre TP, TN, FP, FN, acurácia, precisão e matriz de confusão utilizadas para que possamos comparar os resultados obtidos em diversos modelos matemáticos.

Entretanto quando estamos lidando com modelo de Machine Learning podemos observar também a performance de aprendizado com tempo e experiência de utilização do modelo. Neste post, você vai descobrir os que são as curvas de aprendizado e como elas são utilizadas no diagnóstico de comportamento dos modelos de Machine Learning.


# ROC

A sigla **ROC** significa *“Receiver Operating Characteristic”* e demonstra a capacidade do modelo em distinguir corretamente duas coisas (portanto utilizado para classificação).  Portanto para plotarmos a curva ROC é necessário calcular as probabilidades de cada observação pertencer a classe em questão.

Uma vantagem da curva ROC é podermos observar o ponto de corte (onde o modelo fica estável) e otimizar o desempenho do mesmo, economizando recurso.

O ROC possui os seguintes parâmetros:

 - Sensibilidade
 - Especificidade

A curva ROC traça a Sensibilidade (Taxa de Verdadeiros Positivos) x Especificidade (Taxa de Falsos Positivos)


![](https://scikit-learn.org/stable/_images/sphx_glr_plot_roc_0011.png)

# AUC

Já a **AUC** (*area under the ROC curve*) ou área sobre a curva ROC, ou seja, a derivada da ROC. 

É uma tentativa de resumir a curva ROC em um único valor.

O valor de AUC varia entre 0 e 1,0 e quanto maior for o AUC melhor é a capacidade do modelo em prever a categoria correta.

![](https://www.flai.com.br/wp-content/uploads/2020/06/roc.png)







In this post, you will discover learning curves and how they can be used to diagnose the learning and generalization behavior of machine learning models, with example plots showing common learning problems.


Depois de fazermos a engenharia de dados, ajustes, seleção do modelo e obter alguma saída na forma de probabilidade ou classe a próxima etapa é descobrir quão eficaz é o modelo baseado em alguma métrica.



"Aquilo que não podemos medir, não podemos controlar." `Vicente Falconi`

A métrica explica e compara o desempenho entre os diferentes modelos utilizados no algorítimo.

Um modelo ainda pode apresentar resultados satisfatórios quando avaliados por uma métrica, mas fornecer resultados reais ruins quando colocado em atividade.  A escolha da métrica influencia o desempenho de aprendizado de máquina e como avaliamos a importância de diferentes características nos resultados.

Conforme [documentação](https://scikit-learn.org/stable/modules/model_evaluation.html) do Scikit Learning as métricas podem ser divididas em três grandes blocos: **Classificação, Clustering(agrupamento) e Regressão.**

Neste post irei abordar os métodos mais comuns utilizados em Regressão e Classificação. Caso deseje se aprofundar no assunto a documentação oficial é um ótimo ponto de partida.

# Regressão

## MAE: Mean Absolut Error
    
É a média da diferença do módulo entre o valor original e o valor de previsão obtido pelo modelo;
Não fornece referência para a direção do erro;
Influenciado por outliers;
Atribui o mesmo peso para todas as diferenças.

No gráfico a baixo podemos obsevar o que são as diferenças entre a média `linha` e o valor `pontos`:

![](https://study.com/cimages/multimages/16/heightweightall.png)


Amplitude: 0 ao + infinito.

Quanto menor o erro melhor o modelo.

A fórmula para cálculo é: $$ Mean\ Absolut\ Error = \frac{1}{m} \sum_{i=1}^{n} \left\lvert y_i - \hat{y}_i \right\rvert $$

## MSE: Mean Square Error

Utiliza a média do quadrado do módulo da diferença entre o valor original e o valor de previsão obtido pelo modelo;
Como considera-se o quadrado da diferença penaliza maiores errros;
Muito influenciável por outliers;
Antes de utilizarmos essa métrica devemos eliminar os valores nulos e outliers do dataset;

A fórmula para cálculo é: $$ Mean\ Squared\ Error = \frac{1}{N} \sum_{i=1}^{N} (y_{i} - \hat{y_{i}})^2$$

Amplitude: 0 ao +infinito
Quanto menor o erro melhor o modelo.

## RMSE: Root Mean Squarre Error
Calcula a raíz quadrada dos erros do modelo ao quadrado da diferença entreo o valor original a e o valor de previsão obtido pelo modelo;
RMSE é a raíz do MSE;
Como as métricas RMSE e MSE são elevadas ao quadrado, ambas são muito influenciadas por outliers;

A fórmula para cálculo é: $$ Root\ Mean\ Squared\ Error =\sqrt{ \frac{1}{N} \sum_{i=1}^{N} (y_{i} - \hat{y_{i}})^2}$$ 

Normalmente RMSE é maior ou igual ao MSE.

# Classificação

Como se avalia um modelo de classificação?

Seja trabalhando com classificação binária ou com diversas classes, as métricas relacionadas a modelos de classificação não podem ser os mesmos que usamos em modelos de regressão.

## Accuracy ou Acurácia ou Taxa de Acertos

Contabiliza a taxa de acertos;
Mede o quão próximo o modelo está dos valores reais;
Deve ser utilizada quando temos um dataset com dados balanceados;

Resumidamente podemos falar que acurácia é: $$= \frac{\text{Número de previsões corretas}}{\text{Número total de previsões}}$$

Em relação as previsões podemos dividí-las em:

Tipos de erros:

- **Verdadeiro positivo (*true positive* — TP):** Por exemplo, quando o paciente tem tumor maligno e o modelo classifica como tendo tumor maligno.

- **Falso positivo (*false positive* — FP):** Por exemplo, quando o paciente não tem tumor maligno e o modelo classifica como tendo tumor maligno.

- **Falso negativo (*true negative* — TN)**: Por exemplo, quando o paciente tem tumor maligno e o modelo classifica como não tendo tumor maligno.

- **Verdadeiro negativo (*false negative* — FN):** Por exemplo, quando o paciente não tem tumor maligno e o modelo classifica como não tendo tumor maligno.

![](https://raw.githubusercontent.com/carlosfab/dsnp2/master/img/acuracia.png)

A fórmula para cálculo é: $$ Acuracia = \frac{Number\ of\ correct\ predictions}{Total\ number\ of\ predictions\ made} = \frac{TP + TN}{TP + TN + FP + FN} $$

Como nos foi apresentado a matriz acima podemos nos aprofundar um pouco mais nos conceitos:

## Precisão

Número de exemplos classificados como pertencentes a uma classe que realmente são daquela classe, dividido pela soma destes números e o número de exemplos classificados  nesta classe, ou seja, proporção previsões positivas corretas e todas as previsões positivas.

Representa a capacidade de um modelo em prever a classe negativa.

$$= \frac{TP}{TP+FP} $$

O valor máximo será 1.

## Recall ou sensibilidade

Número de exemplos classificados como pertecentes a uma clase, que realmente são, dividido pelo total de exemplos que pertencem a essa classe. Relação entre previsões positivas corretas e todas as previsões positivas.

Representa a capacidade de um modelo em prever a classe positiva.

Recall = $$ \frac{TP}{TP+FN} $$

## F1-Score

É a média harmônica entre precisão e recall.
Muito utilizado em datasets desbalanceados.
O melhor resultado para o F1-Score é 1.

É calculado por: $$ 2* \frac{precision*recall}{precision+recall} $$

## Matriz de Confusão
A matriz de confusão é uma matriz quadrada onde comparamos os valores verdadeiros de uma classificação com os valores de previsão de alguns modelos. Na diagonal principal dessa matriz quadrada estão os valores corretos e na matriz secundária os erros cometidos pelo modelo.

O algorítimo do Scikit Learn nos permite com um simples comando invocar as métricas aqui apresentadas em forma de sumário:
  {% highlight python %}
  #importando a biblioteca necessária
  from sklearn.metrics import classification_report

  #exemplo de valores
  y_true = [0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0]
  y_pred = [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0]

  #evocando o report de classificação
  print(classification_report(y_true, y_pred))
  {% endhighlight %}

![](http://sigmoidal.ai/wp-content/uploads/2019/10/Screen-Shot-2019-10-07-at-10.02.40.png)


# Conclusão

Tão importante quanto definir um modelo é escolher a métrica correta para avaliá-lo!

O aprendizado de máquina oferece uma grande variedade de maneiras úteis de abordar problemas que, de outra forma, desafiam a solução manual. No entanto, muitas pesquisas atuais de ML sofrem de um distanciamento crescente desses problemas reais. Muitos pesquisadores retiram-se para seus estudos privados com uma cópia do conjunto de dados e trabalhar isoladamente para aperfeiçoar
desempenho algorítmico. Publicação de resultados na comunidade de ML é o fim do processo. Sucesso geralmente não são comunicados de volta ao problema original configuração, ou não em uma forma que possa ser usada.
No entanto, essas oportunidades de impacto real são generalizadas. Os mundos do direito, finanças, política, medicina, educação e muito mais podem se beneficiar de sistemas que podem analisar, adaptar e aceitar (ou pelo menos recomendar) uma ação.


# Motivação

**Data Science na Prática**

O material aqui desenvolvido é parte da provocação feita no curso de Data Science na Prática onde fui desafiado a tentar explicar os passos e ferramentas aplicadas durante a evolução do material.


Todo o material a ser desenvolvido no curso será centralizado no meu [portfolio de projetos](https://github.com/mabittar/Portfolio). 

Para saber mais sobre o curso acesse [https://sigmoidal.ai](https://sigmoidal.ai)

# Fontes

 - Documentação oficial: [Scikit Leaning](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics)
 - ML Wiki: [Link](http://mlwiki.org/index.php/ROC_Analysis)
 - How to use Learning Curves to Diagnose Machine Learning Model Performance: [Link](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)
 - 10 questões de entrevistas: [Link](https://www.flai.com.br/10-questoes-de-data-science-em-entrevistas-de-emprego-da-microsoft/) 